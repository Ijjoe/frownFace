{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import RandomResizedCrop, ToTensor, RandomHorizontalFlip, RandomVerticalFlip, Normalize, ColorJitter, RandomRotation, Resize\n",
    "import torchvision.transforms as T\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install torch torchvision torchaudio\n",
    "#코랩 pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "def checkShape(dataset):\n",
    "    for idx, (image, label) in enumerate(dataset):\n",
    "        print(f'Image {idx+1} shape: {image.shape}  label : {label}')\n",
    "    \n",
    "\n",
    "\n",
    "# 데이터 위치\n",
    "dataset_dir = r\".\\\\archive\\\\NA_Fish_Dataset\\\\\"  #430개\n",
    "# dataset_dir = r\".\\\\archive\\\\Fish_Dataset\\\\\" 정제 데이터 3, 445, 590\n",
    "\n",
    "\n",
    "transform = T.Compose([T.Resize(224,224),T.ToTensor()])\n",
    "\n",
    "# 데이터 호출\n",
    "dataset = ImageFolder(dataset_dir, \n",
    "                      transform = T.ToTensor())  # 이미지 상위폴더 읽기 , PIL 이미지 또는 numpy 배열을 pytorch tensor로 변화 시켜주는  클래스\n",
    "\n",
    "# 각 이미지의 크기 출력 확인\n",
    "checkShape(dataset)\n",
    "\n",
    "# 각 이미지 resize 적용\n",
    "\n",
    "\n",
    "#PyTorch 데이터 로딩 유틸리티의 핵심  dataset을 input으로 넣어주면 여러 옵션(데이터 묶기, 섞기, 알아서 병렬처리)을 통해 batch를 만들어줍니다\n",
    "datasetLoader = DataLoader(dataset, batch_size =32, shuffle = False)\n",
    "\n",
    "\n",
    "print(type(dataset))  #3, 445, 590\n",
    "\n",
    "\n",
    "std, mean = torch.zeros(3),torch.zeros(3)  # 0인 3차원텐서 초기화\n",
    "num_images = len(datasetLoader.dataset)\n",
    "\n",
    "\n",
    "# for images, _ in datasetLoader:\n",
    "#     images = images.view(images.size(0), images.size(1), -1) # 채널 \n",
    "#     mean += images.mean(2).sum(0)\n",
    "#     print(mean )\n",
    "\n",
    "# mean /= num_images\n",
    "\n",
    "\n",
    "#images 텐서의 크기를 변경합니다. view 메서드를 사용하여 텐서를 재구조화합니다.\n",
    "#images.size(0)는 배치 크기, images.size(1)는 채널 수로 유지하고, -1은 남은 차원을 하나의 차원으로 펼친다는 \n",
    "#의미입니다.\n",
    "#예를 들어, 만약 images 텐서의 크기가 [64, 3, 32, 32]라면, view 이후의 크기는 [64, 3, 1024]가 됩니다.\n",
    "\n",
    "#for images, _ in dataset:\n",
    "    #images = images.view(images.size(0), images.size(1), -1)\n",
    "    #mean += images.mean(2).sum(0)\n",
    "#    print(images)\n",
    "#    break\n",
    "\n",
    "#stack expects each tensor to be equal size, but got [3, 2128, 2832] at entry 0 and [3, 768, 1024] at entry 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 3\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import RandomResizedCrop, ToTensor, RandomHorizontalFlip, RandomVerticalFlip, Normalize, ColorJitter, RandomRotation, Resize\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "# 데이터 위치\n",
    "#dataset_dir = r\".\\\\archive\\\\NA_Fish_Dataset\\\\Fish_Dataset\"  #430개\n",
    "dataset_dir = r\".\\\\archive\\\\Fish_Dataset\\\\Fish_Dataset\\\\\" #정제 데이터 3, 445, 590\n",
    "\n",
    "\n",
    "dataset = ImageFolder(dataset_dir, transform = ToTensor())\n",
    "dataset = DataLoader(dataset, batch_size = 64, shuffle = True)\n",
    "\n",
    "std, mean = torch.zeros(3),torch.zeros(3)\n",
    "num_images = len(dataset.dataset)\n",
    "\n",
    "for images, _ in dataset:\n",
    "    images = images.view(images.size(0), images.size(1), -1)\n",
    "    print(images.size(0), images.size(1))\n",
    "    mean += images.mean(2).sum(0)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_img = T.Compose([\n",
    "    \n",
    "    RandomResizedCrop(size=(224, 224)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    ColorJitter(brightness = (0.5, 0.7)),\n",
    "    RandomRotation(degrees = (0, 280)),\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean, std)\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(dataset_dir, transform = transform_img)\n",
    "dataset = DataLoader(dataset, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "images, labels = next(iter(dataset))\n",
    "\n",
    "grid_img = torchvision.utils.make_grid(images).numpy()\n",
    "\n",
    "grid_img = np.transpose(grid_img, (1, 2, 0))\n",
    "grid_img = grid_img / 2 + 0.5 \n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img)\n",
    "plt.axis('off')\n",
    "\n",
    "classes = dataset.dataset.classes\n",
    "print(classes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.80 * len(dataset.dataset))\n",
    "valid_size = int(0.10 * len(dataset.dataset))\n",
    "test_size = len(dataset.dataset) - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset.dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_dataset = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "valid_dataset = DataLoader(valid_dataset, batch_size = 64, shuffle = True)\n",
    "test_dataset = DataLoader(test_dataset, batch_size = 64, shuffle = True)\n",
    "\n",
    "print(f\"Train Dataset Size: {len(train_dataset.dataset)}\")\n",
    "print(f\"Validation Dataset Size: {len(valid_dataset.dataset)}\")\n",
    "print(f\"Test Dataset Size: {len(test_dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exp = int(0.40 * len(train_dataset.dataset))\n",
    "rest_size = len(train_dataset.dataset) - train_exp\n",
    "\n",
    "train_exp, _ = random_split(train_dataset.dataset, [train_exp, rest_size])\n",
    "train_exp = DataLoader(train_exp, batch_size = 64, shuffle = True)\n",
    "\n",
    "print(f\"Experimental Train Dataset Size: {len(train_exp.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(1, 71)]\n",
    "lr = 0.01\n",
    "drop = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import MobileNet_V3_Small_Weights \n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "model = torchvision.models.mobilenet_v3_small(weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for name, child in model.named_children():\n",
    "    if 'layer' in name:\n",
    "        for name2, params2 in child.named_children():\n",
    "            setattr(child, name2, torch.nn.Sequential(params2, torch.nn.Dropout(drop)))\n",
    "            \n",
    "num_layers = len(list(model.parameters()))\n",
    "num_to_freeze = int(num_layers * 0.7)\n",
    "\n",
    "for i, parameter in enumerate(model.parameters()):\n",
    "    \n",
    "    if i < num_to_freeze:\n",
    "        parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = lr)\n",
    "\n",
    "def fit(train_dataset, valid_dataset, model):\n",
    "    \n",
    "    data = []\n",
    "    columns = [\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Validation Loss\", \"Validation Accuracy\"]\n",
    "    dataframe = []\n",
    "    \n",
    "    model.to(device)\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        model.train()\n",
    "        train_total, train_correct, train_loss = 0,0,0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_dataset):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _ , predicted = torch.max(outputs.data, 1)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            avg_train_loss = train_loss / (i+1)\n",
    "            train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "            clear_output(wait = True)\n",
    "            print(f\"Epoch: {epoch}, Train Accuracy: {train_accuracy:.2f}%, Train Loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            total, correct, validation_loss = 0,0,0\n",
    "            \n",
    "            for i, (images, labels) in enumerate(valid_dataset):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _ , prediction = torch.max(outputs.data, 1)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (prediction == labels).sum().item()\n",
    "\n",
    "                avg_validation_loss = validation_loss / (i + 1)\n",
    "                accuracy = 100 * correct / total\n",
    "\n",
    "                data.append([epoch, avg_train_loss, train_accuracy, avg_validation_loss, accuracy])\n",
    "                df = pd.DataFrame(data, columns = columns)\n",
    "                \n",
    "                clear_output(wait = True)\n",
    "                display(df.tail(1))\n",
    "             \n",
    "        dataframe.append([epoch, avg_train_loss, train_accuracy, avg_validation_loss, accuracy])\n",
    "        dataf = pd.DataFrame(dataframe, columns = columns)\n",
    "        clear_output(wait = True)\n",
    "        display(dataf)\n",
    "        \n",
    "fit(train_exp, valid_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "#파일 위치 받기 , 변경사이즈 인자\n",
    "dataset_dir = r\".\\\\archive\\\\NA_Fish_Dataset\\\\Shrimp\\\\00026.png\" #정제 데이터 3, 445, 590\n",
    "\n",
    "def local2Img_resize(dataset_dir,screen=224):\n",
    "    ''' \n",
    "        author : cij methods\n",
    "        dataset_dir : local dirctory files\n",
    "        screen : defult 224 size\n",
    "        \n",
    "        return : resize image , image shape (2 values )\n",
    "    \n",
    "    '''\n",
    "    img= cv2.imread(dataset_dir)\n",
    "    max_dimension = max(img.shape)\n",
    "    retio = screen/max_dimension\n",
    "    re_img=cv2.resize(img, (0,0), fx=retio, fy=retio, interpolation=cv2.INTER_LINEAR)\n",
    "    return re_img,re_img.shape\n",
    "\n",
    "\n",
    "img,sp=local2Img_resize(dataset_dir)\n",
    "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "print(sp)\n",
    "trans_tenser=torch.rand(sp)\n",
    "t=trans_tenser.permute(2, 0, 1)  # 텐서 채널 위치 변경 가능 \n",
    "print(t.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Conv1d(1, 64, kernel_size=3),\n",
    "                                                     nn.MaxPool1d(2), \n",
    "                                                     nn.ReLU(),\n",
    "                                                     nn.Conv1d(64, 128, kernel_size=3),\n",
    "                                                     nn.MaxPool1d(2),\n",
    "                                                     nn.ReLU(),\n",
    "                                                     nn.Flatten(),\n",
    "                                                     nn.Linear(3200,256),\n",
    "                                                     nn.ReLU(),\n",
    "                                                     nn.Linear(256, 10)\n",
    "                                                    )\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "        \n",
    "model = Model().to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
